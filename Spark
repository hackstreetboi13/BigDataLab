sc=sparl.SparkContext()
friends=sc.textFile("")
age_grp = friends.map(lambda x: x.split(',')[2:4]) \
    .map(lambda x: (int(x[0]), int(x[1]))) \
    .groupByKey() \
    .mapValues(lambda x: round(sum(x) / len(x), 2)) \
    .sortByKey()
for age,avg in age_avg.collect():
  print(f"Age: {age} \t Average Friends:{avg}")

--------------------------------------------------------------
lines = sc.textFile('temp.csv')
header = lines.first()
lines = lines.filter(lambda line: line != header) \
             .map(lambda line: line.split(',')) \
             .map(lambda line: (line[0], line[1], line[2], int(line[3])))
minTemp = lines.filter(lambda x: "TMIN" in x[2])
minOverall = minTemp.map(lambda x: x[-1]).min()
print(f"Min. Temp Overall: {minOverall}")
minItemID = minTemp.map(lambda x: (x[0], x[3])).reduceByKey(lambda x, y: min(x, y))
for item,temp in minItemID.collect():
  print(f"ItemID:{item}\t Temp:{temp}")   
minStationID = minTemp.map(lambda x: (x[1], x[3])).reduceByKey(lambda x, y: min(x, y))

maxTemp = lines.filter(lambda x: "TMAX" in x[2])
maxOverall = maxTemp.map(lambda x: x[-1]).max()
print(f"Max. Temp Overall: {maxOverall}")
maxItemID = maxTemp.map(lambda x: (x[0], x[3])).reduceByKey(lambda x, y: max(x, y))
maxStationID = maxTemp.map(lambda x: (x[1], x[3])).reduceByKey(lambda x, y: max(x, y))

---------------------------------------------------------------------
text_file = sc.textFile("words.txt")
counts = text_file.flatMap(lambda line: line.split(" ")) \
                  .map(lambda word: (word, 1)) \
                  .reduceByKey(lambda a, b: a + b)
output = counts.collect()

------------------------------------------------------------------
movies_grp=movies.map(lambda x:x.split('\t')).groupby(lambda x: (x[1],x[2])).mapvalues(len).collect
------------------------------------------------------------------------
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, IntegerType, FloatType, LongType
from pyspark.sql.functions import col, count
spark = SparkSession.builder \
    .appName("Movie Ratings Analysis") \
    .getOrCreate()
schema = StructType([
    StructField("movie_id", IntegerType(), True),
    StructField("user_id", IntegerType(), True),
    StructField("rating", FloatType(), True),
    StructField("timestamp", LongType(), True)
])

df = spark.read.csv("u.data", schema=schema, sep='\t', header=False)
five_star_movies = df.filter(col("rating") == 5)
popular_movies = five_star_movies.groupBy("movie_id") \
    .agg(count("rating").alias("rating_count")) \
    .orderBy(col("rating_count").desc()) \
    .limit(10)

print("Top 10 Most Popular Movies:")
popular_movies.show()

active_users = df.groupBy("user_id") \
    .agg(count("rating").alias("rating_count")) \
    .orderBy(col("rating_count").desc()) \
    .limit(10)


print("Top 10 Most Active Users:")
active_users.show()


spark.stop()
----------------------------------------------------------------------



